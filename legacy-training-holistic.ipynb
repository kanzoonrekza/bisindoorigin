{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for training the holistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembuatan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, log_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "n = 1  # Data augmentation\n",
    "handsOnly = True  # Whether to use only hands or not\n",
    "learning_rate = 0.0001\n",
    "epoch = 300\n",
    "\n",
    "FOLDER_NAME = 'dataset'\n",
    "ALL_CLASSES = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "               'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label map, representing each class as a number\n",
    "label_map = {}\n",
    "for (root, folders, files) in os.walk(FOLDER_NAME):\n",
    "    for foldername in folders:\n",
    "        if foldername in ALL_CLASSES:\n",
    "            label_map[foldername] = ALL_CLASSES.index(foldername)\n",
    "\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all datset data with its label and put it in a list\n",
    "sequence, label = [], []\n",
    "target_length = 14\n",
    "for (root, folders, files) in os.walk(FOLDER_NAME):\n",
    "    total_file = 0\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(os.path.relpath(\n",
    "            root, FOLDER_NAME), filename)\n",
    "        if (filename.endswith('.npy') and os.path.split(file_path)[0] in ALL_CLASSES):\n",
    "            res = np.load(f'{FOLDER_NAME}/{file_path}')\n",
    "            for _ in range(target_length-res.shape[0]):\n",
    "                res = np.vstack((res, res[-1, :]))\n",
    "            if(handsOnly):\n",
    "                res = res[:, -126:]\n",
    "            sequence.append(np.array(res))\n",
    "            label.append(label_map[os.path.basename(root[-1])])\n",
    "            total_file += 1\n",
    "    print(f\"Total files: {total_file} --- {root}\")\n",
    "\n",
    "print(np.array(sequence).shape)\n",
    "print(np.array(label).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.concatenate([sequence] * n, axis=0)\n",
    "label = np.concatenate([label] * n, axis=0)\n",
    "\n",
    "\n",
    "print(np.array(sequence).shape)\n",
    "print(np.array(label).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sssplit import balanced_train_test_split\n",
    "X_train, X_test, y_train, y_test = balanced_train_test_split(np.array(sequence),\n",
    "                                                             np.array(label).astype(int))\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_phase = str(np.array(\n",
    "    sequence).shape[2]) + \"-\" + str(n) + \"X-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join('Logs', training_phase)\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "print(training_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,\n",
    "               activation='tanh', input_shape=(14, np.array(sequence).shape[2])))\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(np.array(ALL_CLASSES).shape[0], activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'{log_dir}/action.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epoch,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[tb_callback, early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "classes = np.unique(y_true)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "loss = log_loss(y_true, y_pred, labels=classes)\n",
    "\n",
    "history_data = history.history\n",
    "start_epoch = max(0, len(history_data['loss']) - 20)\n",
    "stopped_epoch = early_stopping.stopped_epoch\n",
    "best_epoch = stopped_epoch - early_stopping.patience\n",
    "\n",
    "# Redirect stdout to a string buffer\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "print(f\"Training Phase: {training_phase}\\n\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "print(\"\\nLast 20 Epochs:\")\n",
    "for epoch in range(start_epoch, len(history_data['loss'])):\n",
    "    loss = history_data['loss'][epoch]\n",
    "    accuracy = history_data.get('categorical_accuracy', [None])[epoch]\n",
    "    val_loss = history_data['val_loss'][epoch]\n",
    "    val_accuracy = history_data.get('val_categorical_accuracy', [None])[epoch]\n",
    "    print(f\"Epoch {epoch + 1}: loss = {loss:.8f}, categorical_accuracy = {accuracy:.8f} | val_loss = {val_loss:.8f}, val_categorical_accuracy = {val_accuracy:.8f}\")\n",
    "\n",
    "print(f\"\\nStopped at epoch: {stopped_epoch + 1}\")\n",
    "print(f\"Best epoch (saved weights): Epoch {best_epoch + 1}\")\n",
    "\n",
    "print(\"\\nEarly Stopping Configuration:\")\n",
    "print(f\"Patience: {early_stopping.patience}\")\n",
    "print(f\"Monitored value: {early_stopping.monitor}\")\n",
    "print(f\"Baseline: {early_stopping.baseline}\")\n",
    "\n",
    "report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "sys.stdout = old_stdout\n",
    "output = buffer.getvalue()\n",
    "\n",
    "# Save the output to a uniquely named text file in the Logs directory\n",
    "log_filename = f'{log_dir}/summary.txt'\n",
    "\n",
    "with open(log_filename, 'w') as f:\n",
    "    f.write(output)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, square=True, linewidths=0,\n",
    "            xticklabels=[f'{ALL_CLASSES[cls]}' for cls in np.unique(y_true)],\n",
    "            yticklabels=[f'{ALL_CLASSES[cls]}' for cls in np.unique(y_true)])\n",
    "\n",
    "\n",
    "# Add labels for axes\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Save the figure as a PDF\n",
    "plt.savefig(f'{log_dir}/confusion_matrix.pdf', format='pdf')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
